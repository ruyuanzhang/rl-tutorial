{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load REINFORCE.py\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Define Policy\n",
    "## 1.1 init\n",
    "\n",
    "* Softmax in action preference\n",
    "* $h(a,s,\\theta)$ -- ANN\n",
    "  * 1 hidden layer, of 128 neurons\n",
    "  * Activation Func, relu\n",
    "  * Output layer, Softmax (softmax in action preference!)\n",
    "\n",
    "# 1.2 train_net, update rules for $\\theta$\n",
    "$$ G \\leftarrow \\sum^T_{k=t+1} \\gamma ^{k-t-1}R_k$$\n",
    "$$ \\theta \\leftarrow \\theta + \\alpha \\gamma ^t G \\nabla \\ln \\pi (A_t|S_t, \\theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []     # data of 1 entire episode\n",
    "\n",
    "        # I. ================================\n",
    "        # 1.1 Initial Policy network (1)\n",
    "\n",
    "        # ===================================\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # I. ================================\n",
    "        # 1.1 Initial Policy network (2)\n",
    "\n",
    "        # ===================================\n",
    "\n",
    "        return x\n",
    "      \n",
    "    def put_data(self, item):     # adding data of 1 step\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def train_net(self):\n",
    "        self.optimizer.zero_grad()\n",
    "        G = 0\n",
    "\n",
    "        # III. ==============================\n",
    "        # 3. Loop for each step of the episode, update theta\n",
    "\n",
    "        # ===================================\n",
    "\n",
    "        self.optimizer.step()\n",
    "        self.data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check output of policy network, shoule be 2-dim tensor\n",
    "env = gym.make('CartPole-v1')\n",
    "s = env.reset()[0]\n",
    "pi = Policy()\n",
    "pi(torch.Tensor(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tensor([0.4690, 0.5310])\n",
    "output\n",
    "\n",
    "# introduce func: how to sample action\n",
    "m = Categorical(output)\n",
    "print(m)\n",
    "a = m.sample()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get action a, from tensor a\n",
    "a.item()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Sample, Observe cartpole\n",
    "1 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "pi = Policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to sample next state\n",
    "s_prime, r, done, _, _ = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, _ = env.reset()\n",
    "done = False\n",
    "t = 0\n",
    "\n",
    "while not done:\n",
    "    # 2.0 show\n",
    "    plt.plot([-5, 5], [0,0], c='black')\n",
    "    plt.scatter([-2.4, 2.4], [0,0], c='black')\n",
    "    plt.scatter(s[0], 0, marker='^', c='#FF4500', s = 400)\n",
    "    plt.plot([s[0], s[0] + 8*np.tan(s[1]*10/180*np.pi)], [0,8])\n",
    "    plt.xlim(-5,5)\n",
    "    plt.ylim(-1,10)\n",
    "    clear_output(True)\n",
    "    plt.show()\n",
    "    plt.pause(0.005)\n",
    "\n",
    "    # II. ===============================\n",
    "    # 2.1 Sample action, according to pi\n",
    "\n",
    "\n",
    "    # 2.2 observe next step\n",
    "\n",
    "    # ===================================\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode finished after {} timesteps\".format(t+1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Sample, and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "pi = Policy()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "li_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_epi in range(10000):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done: # CartPole-v1 forced to terminates at 500 step.\n",
    "\n",
    "        # II. ===============================\n",
    "        # 2.1 Sample action, according to pi\n",
    "\n",
    "\n",
    "        # 2.2 observe next step,\n",
    "        # and Record data of 1 step with \"pi.put_data\", putting (r, prob[a])\n",
    "\n",
    "        # ===================================\n",
    "        pass\n",
    "        \n",
    "    # 3. Train net\n",
    "    pi.train_net()\n",
    "    \n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0\n",
    "    li_score.append(score)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(li_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
